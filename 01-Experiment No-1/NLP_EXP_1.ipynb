{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# To implement tokenization by word"
      ],
      "metadata": {
        "id": "XGP92BOwqa1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvlkzDN1qVrY",
        "outputId": "f496c17d-bf78-4b68-fc2b-26c8e3976f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_by_word(text):\n",
        "    return nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "dBhVPWh2qhq1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6990f8a5",
        "outputId": "fed2d9d8-75b0-4c46-c356-4d66f8dc7052"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28197f60",
        "outputId": "676ed33b-09a0-4f67-d5a9-b7c8cc8eda5a"
      },
      "source": [
        "sample_text = \"Hello! My name is Prathamesh Jadhav.\"\n",
        "tokens = tokenize_by_word(sample_text)\n",
        "print(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'My', 'name', 'is', 'Prathamesh', 'Jadhav', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenization using NLTK"
      ],
      "metadata": {
        "id": "Qvk5fuDYq9-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the required data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello! My name is Prathamesh Jadhav. I am learning Natural Language Processing using Python.\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"Sentence Tokenization:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2ezKlEKrGpE",
        "outputId": "f45af390-bd8d-4cf5-8be8-4b407a5eed9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "1. Hello!\n",
            "2. My name is Prathamesh Jadhav.\n",
            "3. I am learning Natural Language Processing using Python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization + Count"
      ],
      "metadata": {
        "id": "HHBjTKnpraLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Count Program using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download required tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Paragraph related to AI & Data Science\n",
        "paragraph = \"\"\"\n",
        "Artificial Intelligence and Data Science are transforming industries by enabling machines to learn from data.\n",
        "These technologies help in predictive analytics, automation, and making intelligent decisions.\n",
        "Data Science involves collecting, cleaning, and analyzing large volumes of data to uncover hidden patterns and insights.\n",
        "\"\"\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentence Tokenization:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sent}\")\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)\n",
        "\n",
        "# Count sentences and words\n",
        "print(\"\\nTotal Number of Sentences:\", len(sentences))\n",
        "print(\"Total Number of Words:\", len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itI5HUPZreu8",
        "outputId": "72f8d609-da6f-4925-ea4a-c307c13f12fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "1. \n",
            "Artificial Intelligence and Data Science are transforming industries by enabling machines to learn from data.\n",
            "2. These technologies help in predictive analytics, automation, and making intelligent decisions.\n",
            "3. Data Science involves collecting, cleaning, and analyzing large volumes of data to uncover hidden patterns and insights.\n",
            "\n",
            "Word Tokenization:\n",
            "['Artificial', 'Intelligence', 'and', 'Data', 'Science', 'are', 'transforming', 'industries', 'by', 'enabling', 'machines', 'to', 'learn', 'from', 'data', '.', 'These', 'technologies', 'help', 'in', 'predictive', 'analytics', ',', 'automation', ',', 'and', 'making', 'intelligent', 'decisions', '.', 'Data', 'Science', 'involves', 'collecting', ',', 'cleaning', ',', 'and', 'analyzing', 'large', 'volumes', 'of', 'data', 'to', 'uncover', 'hidden', 'patterns', 'and', 'insights', '.']\n",
            "\n",
            "Total Number of Sentences: 3\n",
            "Total Number of Words: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}